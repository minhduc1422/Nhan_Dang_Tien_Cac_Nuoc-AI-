import os
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain_community.embeddings import GPT4AllEmbeddings
from langchain_community.vectorstores import FAISS
from dotenv import load_dotenv

# Load API key từ file .env
load_dotenv(override=True)
api_key = os.getenv("GEMINI_API_KEY")

if not api_key:
    raise ValueError("API Key không hợp lệ. Vui lòng kiểm tra file .env hoặc đặt GOOGLE_API_KEY.")

vector_db_path = "D:/thuc_tap_tot_nghiep/vectorstores/db_faiss"

def load_llm(api_key):
    try:
        llm = ChatGoogleGenerativeAI(
            model="gemini-pro",  # Chọn mô hình Gemini
            google_api_key=api_key,
            temperature=0.01,
            max_output_tokens=1024
        )
        return llm
    except Exception as e:
        raise RuntimeError(f"Lỗi khi khởi tạo mô hình: {e}")

def create_prompt(template):
    return PromptTemplate(template=template, input_variables=["context", "question"])

def create_qa_chain(prompt, llm, db):
    return RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=db.as_retriever(search_kwargs={"k": 3}),
        return_source_documents=False,
        chain_type_kwargs={'prompt': prompt}
    )

def read_vectors_db():
    embedding_model = GPT4AllEmbeddings(model_file="model/all-MiniLM-L6-v2-f16.gguf")
    db = FAISS.load_local(vector_db_path, embedding_model, allow_dangerous_deserialization=True)
    return db

try:
    db = read_vectors_db()
    llm = load_llm(api_key)
    template = """<|im_start|>system\n Sử dụng thông tin sau đây để trả lời câu hỏi. Nếu bạn không biết câu trả lời, hãy nói tôi cũng không biết nữa chịu, đừng cố tạo ra.\n
{context}<|im_end|>\n<|im_start|>user\n{question}<|im_end|>\n<|im_start|>assistant"""
    prompt = create_prompt(template)
    llm_chain = create_qa_chain(prompt, llm, db)
    
    # Cho phép người dùng nhập câu hỏi
    while True:
        question = input("Nhập câu hỏi của bạn (hoặc nhập 'exit' để thoát): ")
        if question.lower() == 'exit':
            break
        response = llm_chain.invoke({"câu hỏi": question})
        print("Trợ lý AI:", response)
except Exception as e:
    print(f"Đã xảy ra lỗi: {e}")
